<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"> <!--<![endif]-->
<head>

  <!-- Basic Page Needs
  ================================================== -->
  <meta charset="utf-8">
  <title>Elliot Greenwood's Personal Page</title>
  <meta name="description" content="Personal Page">
  <meta name="author" content="Elliot Greenwood">


  <!-- Mobile Specific Metas
  ================================================== -->
  <!--<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">-->


  <!-- CSS
  ================================================== -->
  <style>
  #top,a,abbr,acronym,address,applet,article,aside,audio,b,big,blockquote,body,canvas,caption,center,cite,code,dd,del,details,dfn,div,dl,dl dt,dt,em,embed,fieldset,figcaption,figure,footer,form,h1,h2,h3,h4,h5,h6,header,hgroup,html,i,iframe,img,ins,kbd,label,legend,li,mark,menu,nav,object,ol,output,p,pre,q,ruby,s,samp,section,small,span,strike,strong,sub,summary,sup,table,tbody,td,tfoot,th,thead,time,tr,tt,u,ul,var,video{margin:0;padding:0;border:0;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,menu,nav,section{display:block}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:after,blockquote:before,q:after,q:before{content:'';content:none}::selection{color:#050505;background:#c7c7c7}::-moz-selection{color:#050505;background:#c7c7c7}html{color:#272727;font-family:Calluna,Georgia,Times,"Times New Roman",serif;font-variant-ligatures:additional-ligatures;-webkit-font-variant-ligatures:additional-ligatures;text-rendering:optimizeLegibility;font-kerning:normal}p{margin:0 0 18px;font-size:1em;line-height:1.4em;text-align:justify;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}p.lead{font-size:1em;line-height:1.5em}h1{font-size:3em;line-height:1.125em}h2{font-size:2.25em;line-height:1.5em}#top,h3{font-size:1.5em;line-height:2em;font-weight:700}h4{font-size:1em;line-height:1.5em;font-weight:700}h5{line-height:1.5em;font-weight:700}a,a:visited{color:#050505}a:focus,a:hover{text-decoration:none}a:focus.purple,a:hover.purple{color:#cd9aba}a:focus.blue,a:hover.blue{color:#8baecc}a:focus.green,a:hover.green{color:#a4c39c}a:focus.yellow,a:hover.yellow{color:#e3cf56}nav a,nav a:visited{text-decoration:none;color:#050505}.classification header>nav a:focus,.classification header>nav>ul>li>a:hover{color:#881058}.regression header>nav a:focus,.regression header>nav>ul>li>a:hover{color:#0e5f9b}.techniques header>nav a:focus,.techniques header>nav>ul>li>a:hover{color:#277512}.summary header>nav a:focus,.summary header>nav>ul>li>a:hover{color:#dbbd0b}.home header>nav a:focus,.home header>nav>ul>li>a:hover,.listen header>nav a:focus,.listen header>nav>ul>li>a:hover{color:#c7c7c7}em{font-style:italic}dl dt,h1,h2,strong{font-weight:700}code,pre{font-family:prestige-elite-std,Menlo,monospace}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}blockquote{margin:10px 50px 10px 0;padding-left:15px;border-left:3px solid #c7c7c7}.classification blockquote{color:#881058;border-color:#881058}.regression blockquote{color:#0e5f9b;border-color:#0e5f9b}.techniques blockquote{color:#277512;border-color:#277512}.summary blockquote{color:#dbbd0b;border-color:#dbbd0b}header *{font-family:proxima-nova,HelveticaNeue,"Helvetica Neue",Helvetica,Verdana,Arial,sans-serif;font-weight:900}.left{text-align:left}.right{text-align:right}.center,figure,figure figcaption,figure figcaption p{text-align:center}.f-l{float:left}#top,.f-r{float:right}.c-l{clear:left}.c-r{clear:right}.c-b{clear:both}.i-b{display:inline-block}.a-c,header ul{text-transform:uppercase}.s-c{font-variant:small-caps}body{background:#f3f3f3}body.classification{background:#faf4f8}body.regression{background:#edf1f6}body.techniques{background:#f3f6f2}body.summary{background:#f9f9f4}.container{margin:0 auto;width:960px}.post-html{width:640px;margin:0 auto;padding:0 0 36px}.post-html ol{list-style:decimal-leading-zero}.post-html ol li{padding-bottom:10px;line-height:1.4em}.post-html ul{float:none;list-style:disc}.post-html ul li{padding-bottom:10px;line-height:1.4em}.post-html ul li>ul{padding-left:24px;padding-top:6px}.post-html ul li>ul>li{list-style:circle;padding-bottom:4px}header{margin:10px 0 26px;padding:0 40px}header *{display:inline-block}.titles{position:relative;margin-top:-460px}footer{height:18px;margin:18px 0 36px;display:block;width:960px}figure{padding:0 0 18px;margin:0 auto}figure img{padding:9px;width:520px}figure figcaption{width:100%;font-size:75%;color:#949494;position:relative}#top{position:fixed;bottom:24px;right:12%}nav{display:inline-block;height:80px;float:left}nav li{cursor:pointer}header ul{float:left}header ul>li{display:inline-block;padding:32px 0 0 24px;position:relative}header ul>li>ul{float:right;height:24px;margin:-9px 16px 0}header ul>li>ul li{padding:0 16px 0 0}header ul>li>ul li .classification{color:#cd9aba}header ul>li>ul li .regression{color:#8baecc}header ul>li>ul li .techniques{color:#669c57}header ul>li>ul li .summary{color:#e3cf56}header ul>li>ul li:hover .classification{color:#ab568a}header ul>li>ul li:hover .regression{color:#4d86b4}header ul>li>ul li:hover .techniques{color:#277512}header ul>li>ul li:hover .summary{color:#dbbd0b}dl{clear:both}dl .defn-container{float:left;width:480px;height:80px;border-left:4px solid #272727;padding:9px 0 9px 9px;margin:9px;background:#fff;-webkit-border-top-right-radius:3px;-webkit-border-bottom-right-radius:3px;-moz-border-radius-topright:3px;-moz-border-radius-bottomright:3px;border-top-right-radius:3px;border-bottom-right-radius:3px}dl .defn-container.classification{border-color:#881058}dl .defn-container.regression{border-color:#0e5f9b}dl .defn-container.techniques{border-color:#277512}dl .defn-container.summary{border-color:#dbbd0b}dl dt{margin-bottom:18px;width:440px}dl dd{font-size:.75em;line-height:1.125em;width:440px}dl .defn-arr{padding-top:32px;float:right;display:inline-block}dl .defn-arr a{text-decoration:none}dl:after{clear:left}.vertical-bar{position:relative;top:5px;height:24px;width:4px;margin-right:6px;display:inline-block}.vertical-bar.tall{position:static;height:80px;margin:0 16px}.vertical-bar.dark-grey{background:#272727}.vertical-bar.classification{background:#cd9aba}.vertical-bar.regression{background:#8baecc}.vertical-bar.techniques{background:#669c57}.vertical-bar.summary{background:#e3cf56}a.purple:hover .vertical-bar{background:#ab568a}a.blue:hover .vertical-bar{background:#4d86b4}a.green:hover .vertical-bar{background:#277512}a.yellow:hover .vertical-bar{background:#dbbd0b}hr{border:none;border-top:1px solid #3e3e3e}.classification hr{border-color:#881058}.regression hr{border-color:#0e5f9b}.techniques hr{border-color:#277512}.summary hr{border-color:#dbbd0b}table{border-collapse:collapse;border-spacing:0;empty-cells:show;border:2px solid #3e3e3e;margin:15px auto}.classification table{border-color:#881058}.regression table{border-color:#0e5f9b}.techniques table{border-color:#277512}.summary table{border-color:#dbbd0b}table caption{color:#000;font:italic 85%/1 arial,sans-serif;padding:1em 0;text-align:center}table td,table th{border-width:0 0 0 1px;font-size:inherit;text-align:center;margin:0;overflow:visible;padding:.5em 1em}table td:first-child,table th:first-child{border-left-width:0}table thead{text-align:left;vertical-align:bottom}.classification table thead{background-color:#881058;color:#faf4f8}.regression table thead{background-color:#0e5f9b;color:#edf1f6}.techniques table thead{background-color:#277512;color:#f3f3f3}.summary table thead{background-color:#dbbd0b;color:#f3f3f3}table td{background-color:transparent}.classification table tr:nth-child(2n) td{background-color:#cd9aba}.regression table tr:nth-child(2n) td{background-color:#8baecc}.techniques table tr:nth-child(2n) td{background-color:#669c57}.summary table tr:nth-child(2n) td{background-color:#e3cf56}@-webkit-keyframes fadeInLeft{0%{opacity:0;-webkit-transform:translate3d(-16px,0,0);transform:translate3d(-16px,0,0)}100%{opacity:1;-webkit-transform:none;transform:none}}@keyframes fadeInLeft{0%{-webkit-transform:translate3d(-16px,0,0);transform:translate3d(-16px,0,0)}100%{opacity:1;-webkit-transform:none;transform:none}}.fadeInLeft{-webkit-animation:fadeInLeft .4s;animation:fadeInLeft .4s;-webkit-animation-fill-mode:both}
  </style>

  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->


</head>
<body class="regression">
<div id="post-regression" class="post-html"><h1>Regression</h1>

<p>Music emotion recognition is a complicated problem to resolve, but a lot of research is going into using different methodologies to reason about it. This article will focus on one methodology, <em>regression</em>, to help a machine effectively understand emotions connected to different kinds of music. Regression specifies different emotions as having a certain mood (determined by a Valence level) and intensity (determined by an Arousal level). For example, an excited mood will have a positive valence level with a very high arousal level while a sleepy mood will have a slightly negative valence level with a low arousal level. These moods are associated with specific segments of a 2D plane, with valence on the horizontal axis and arousal on the vertical axis. (see <a href="#Fig41">Fig. 4.1</a>) We shall refer to this as the arousal-valence plane of emotion (Thayer, 1989). Using regression, it is a machine's job to establish the correct emotions on this plane to their respective musical samples. Another important aspect of the machine which is in early days of research, is the ability to improve how it analyses emotions over time. Throughout this article I will look into the importance of communication between machines and humans, Psychological issues related to emotions, properties of regression, a comparison of techniques that use regression and an analysis of how a MER machine can improve its evaluation process.</p>

<h4 id="Fig41" class="center">Valence-Arousal Diagram</h4>

<p></p><figure markdown="1">
<img src="/MERMachineLearning/assets/images/VADiagram.png" alt="Thayer's two-dimensional emotion plane"><p></p>

<figcaption>

<p>Figure 4.1: Adapted from: Russell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social Psychology, 39, 1161–1178.</p>

</figcaption>

<p></p></figure><p></p>

<h3>Miscommunication</h3>

<p>An interesting issue to consider when labelling a music sample with an emotion, is the fact that not everyone will label the same music with the same emotion. For example, elderly people may not appreciate hip hop music as much as their younger counter parts, thus there is a concern that MER machines will provide conflicting emotions for certain samples. Yang et al. (2008) noted problems with applying certain adjectives to music as they can be ambiguous with varying interpretations. Using ambiguous emotions will lead to miscommunication between the machine and the user when it comes to selecting a music sample for a specific emotion.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref">1</a></sup></p>

<h3>Psychological Impact</h3>

<p>Another problem we have to consider is the fact that different people will perceive different emotions for a music sample based on experiences they had whilst listening to the music. Michael Stevens (2013) made a video noting the effects of nostalgia and how music triggers different emotions because of emotive memories. He goes on to state: “Music is initially processed in the same regions of the brain that process memories and emotion…” <sup id="fnref:1"><a href="#fn:1" class="footnote-ref">2</a></sup>. Because of this if something traumatic happens to you whilst listening to a music sample that could be happy in nature, you may perceive it as sad and distressing because it unearths memories you would rather not think about. This is a very subjective topic which machines will not be able to comprehend very well, even humans don’t fully understand the link between music, emotion and memories. These psychological issues make regression complicated to use as machines currently can’t perceive abstract concepts such as memories, thus they will struggle to adapt to problems involving them.</p>

<h3>Continuous vs. Discrete</h3>

<p>When it comes to working with the arousal-valence plane there are a few problems researchers have to address. We already discussed the problem of emotions being ambiguous in nature. For example, the top right quadrant will contain areas for emotions such as happy, excited and thrilled. Despite their similarities they are still distinctively different emotions. Because of this, when working with the selection of a music sample's emotion on the arousal-valence plane some researchers consider a continuous spectrum of emotions (Yang et al., 2008). This means that each point can be measured in continuous quantities rather discrete one, thus greatly easing the ambiguity issue.</p>

<h3>Time Intervals</h3>

<p>The emotion of music can greatly vary over time. A piece of music could express a sad emotion and change to a joyful one later on in the song. This problem can be remedied if we consider the emotions of different intervals of a piece of music rather than one emotion for the whole song <sup id="fnref:6"><a href="#fn:6" class="footnote-ref">3</a></sup>. The <abbr title="Valence-Arousal">VA</abbr> values, corresponding to points on Thayer’s arousal-valence place, of set time intervals of a music sample (say 5 seconds) is recorded throughout the duration of the song. A song with a duration of 60 will have 12 associated <abbr title="Valence-Arousal">VA</abbr> values for each time interval (If a 5 second interval is used). A mean <abbr title="Valence-Arousal">VA</abbr> value is calculated from each interval’s <abbr title="Valence-Arousal">VA</abbr> values and used to calculate the overall emotion of the song.</p>

<h3>Support Vector Regression</h3>

<p>There are quite a few systems in place that uses regression to analyse emotion in music. Han B. et al. (2009) researched the benefits of using a support vector regression (<abbr title="Support Vector Regression.">SVR</abbr>) technique to perceive the emotion of music. They took a categorical approach by splitting the arousal-valence axis into 11 categories of emotions <sup id="fnref:5"><a href="#fn:5" class="footnote-ref">4</a></sup>. Their <abbr title="Support Vector Regression.">SVR</abbr> system analyses seven musical features: pitch, tempo, volume, tonality, key, rhythm and harmonics. The <abbr title="Support Vector Regression.">SVR</abbr> system analyses a specific music sample’s musical features and maps each feature to certain emotion categories on the arousal-valence axis such as calm, nervous, exited and sad. The system then creates vectors out of the evaluated musical features depending on the emotion each feature expressed. These vectors are mapped onto a polar coordinate system (similar in nature to the Cartesian arousal-valence axis). Each vector is associated with a distance from the origin (0, 0) and an angle from the x axis to the vector. The <abbr title="Support Vector Regression.">SVR</abbr> is able to effectively analyse the mood of musical features by evaluating the categories on the arousal-valence axis their vector representation fall upon.
</p><figure markdown="1">
<img src="/MERMachineLearning/assets/images/SVRFlowChart.png" alt="Flow Chart"><p></p>

<figcaption>
  Figure 4.2: SVR flow chart
</figcaption>

<p></p></figure><p></p>

<h3>Active Machine Learning</h3>

<p>Support vector regression could be improved if more sophisticated machine learning features were implemented into <abbr title="Support Vector Regression.">SVR</abbr> systems. As these systems constantly analyse emotions of a sample’s musical features, it could be possible for the system to memorise results of music emotion evaluations. Next time the system tries to analyse a sample it could reference a bank of memorised results to produce more accurate predictions of any sample’s emotion. This method of machine learning is usually referred to as Active learning. Simon Tong (2001) investigated the many benefits of active learning. He notes that when analysing large sets of data, musical features in the case of <abbr title="Support Vector Regression.">SVR</abbr>, it can be quite time consuming for a machine to look through them all <sup id="fnref:7"><a href="#fn:7" class="footnote-ref">5</a></sup>. Active learning features can allow machines to query results from previous data to evaluate future data. This vastly improves a MER machine as it is able to produce results with greater speed and learn how to analyse data more effectively at the same time.</p>

<h3>Linear Regression</h3>

<p>Linear regression (<abbr title="Linear Regression.">LR</abbr>) is another technique that utilises regression. Yu-An Chen et al. (2014) used a linear regression based learner for their MER machine <sup id="fnref:3"><a href="#fn:3" class="footnote-ref">6</a></sup>. As noted before, different people can give varying emotions for the same music. Because of this <abbr title="Linear Regression.">LR</abbr> usually models the emotions a music sample expresses through Gaussian distribution. Techniques involving Gaussian distributions is another way of mapping musical signals to their respective points on an arousal-valence plane. Yang et al (2012) produced an approach called Acoustic Emotion Gaussians (<abbr title="Acoustic Emotion Gaussians.">AEG</abbr>) that can be used to train a <abbr title="Linear Regression.">LR</abbr> machine <sup id="fnref:9"><a href="#fn:9" class="footnote-ref">7</a></sup>. This approach creates a set of acoustic features that maps on to a set of <abbr title="Valence-Arousal">VA</abbr> emotion values with the same number of elements. The acoustic features are mapped into a vector using <abbr title="Acoustic Emotion Gaussians.">AEG</abbr> which is then mapped onto an arousal-valence plane. The coordinates on the plane make up the <abbr title="Valence-Arousal">VA</abbr> emotion values of the second set which is used to produce the overall emotion of the song.</p>

<h3>Regression Techniques Comparison</h3>

<p>I have looked into 2 techniques that use regression to implement MER machines. They both strive to achieve the same goal with different processes. They are similar in the fact they both use a continuous model and Thayer’s arousal-valence plane. The table below will list a few advantages tied to these techniques.</p>

<table>
<thead>
<tr>
  <th>Learner technique</th>
  <th>Emotion Space</th>
  <th>Advantages</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Linear Regression</td>
  <td>Continuous</td>
  <td>Works well with speech recognition when little data input is used.</td>
</tr>
<tr>
  <td>Support Vector Regression</td>
  <td>Continuous</td>
  <td>Active learning can be utilised to a <abbr title="Support Vector Regression.">SVR</abbr> learner to improve its evaluation process</td>
</tr>
</tbody>
</table>

<h3>Conclusion</h3>

<p>Regression has proven itself to be a very useful methodology when it comes to giving machines the necessary tools they need to reason about emotions and music. Regression helps progress the applications of MER machines by implementing features such as the analysis of emotions in specific time intervals and the use of an unambiguous arousal-valence plane. The techniques that utilise regression show the possibility for many advancements in the incorporation of machine learning algorithms. Such algorithms that could use active learning to train machines to improve themselves, are still in early days of development. But research into this field shows there are endless possibilities of what could be achieved with regression and music emotion recognition.</p>

<h2>References</h2>

<div class="footnotes">
<hr>
<ol>

<li id="fn:2">
<p>Yang , Y., Lin , Y., Su , Y. &amp; Chen , H. (2008) A Regression Approach to Music Emotion Recognition. <em>IEEE Transactions on Audio, Speech, and Language Processing.</em> 16 (2), 1-1-10.&nbsp;<a href="#fnref:2" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:1">
<p>Stevens, M. (2013) <em>Why Do We Feel Nostalgia?</em> [Video] Available from: <a href="https://www.youtube.com/watch?v=coGfGmOeLjE" target="_blank">https://www.youtube.com/watch?v=coGfGmOeLjE</a> [Accessed 15/02/2015].&nbsp;<a href="#fnref:1" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:6">
<p>Kim, Y., Schmidt , E., Migneco, R., Morton, B., Richardson , P., Scott , J., Speck , J. &amp; Turnbull , D. (2010) <em>Music Emotion Recognition: A State Of The Art Review.</em> Computer Science. Drexel University.&nbsp;<a href="#fnref:6" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:5">
<p>Han , B., Rho , S., Dannenberg , R. &amp; Hwang , E. (2009) Smers: Music emotion recognition using support vector regression. <em>10th International Society for Music Information Retrieval Conference.</em>.&nbsp;<a href="#fnref:5" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:7">
<p>Tong , S. (2001) <em>Active Learning: Theory and Applications.</em> Philosophy. Stanford university.&nbsp;<a href="#fnref:7" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:3">
<p>Chen, Y., Wang, J., Yang, Y. &amp; Chen, H. (2014) Linear regression-based adaptation of music emotion recognition models for personalization. <em>2014 IEEE International Conference on Acoustic, Speech and Signal Processing.</em> [Online], 09/03/2015. Available from: <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6853979&amp;tag=1" target="_blank">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6853979&amp;tag=1</a> [Accessed 09/03/2015].&nbsp;<a href="#fnref:3" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:9">
<p>Wang, J., Yang, Y., Wang, H. &amp; Jeng, S. (2012) <em>The acoustic emotion Gaussians model for emotion-based music annotation and retrieval.</em> Department of Electrical Engineering. National Taiwan University.&nbsp;<a href="#fnref:9" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

</ol>
</div>
</div>
</body>
</html>
