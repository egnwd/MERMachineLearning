<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"> <!--<![endif]-->
<head>

  <!-- Basic Page Needs
  ================================================== -->
  <meta charset="utf-8">
  <title>Elliot Greenwood's Personal Page</title>
  <meta name="description" content="Personal Page">
  <meta name="author" content="Elliot Greenwood">


  <!-- Mobile Specific Metas
  ================================================== -->
  <!--<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">-->
<script src ="//use.typekit.net/xvl2gac.js"></script>
<script>try{Typekit.load();}catch(e){}</script>

  <!-- CSS
  ================================================== -->
  <style>
  #top,a,abbr,acronym,address,applet,article,aside,audio,b,big,blockquote,body,canvas,caption,center,cite,code,dd,del,details,dfn,div,dl,dl dt,dt,em,embed,fieldset,figcaption,figure,footer,form,h1,h2,h3,h4,h5,h6,header,hgroup,html,i,iframe,img,ins,kbd,label,legend,li,mark,menu,nav,object,ol,output,p,pre,q,ruby,s,samp,section,small,span,strike,strong,sub,summary,sup,table,tbody,td,tfoot,th,thead,time,tr,tt,u,ul,var,video{margin:0;padding:0;border:0;font:inherit;vertical-align:baseline}article,aside,details,figcaption,figure,footer,header,hgroup,menu,nav,section{display:block}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:after,blockquote:before,q:after,q:before{content:'';content:none}::selection{color:#050505;background:#c7c7c7}::-moz-selection{color:#050505;background:#c7c7c7}html{color:#272727;font-family:Calluna,Georgia,Times,"Times New Roman",serif;font-variant-ligatures:additional-ligatures;-webkit-font-variant-ligatures:additional-ligatures;text-rendering:optimizeLegibility;font-kerning:normal}p{margin:0 0 18px;font-size:1em;line-height:1.4em;text-align:justify;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto}p.lead{font-size:1em;line-height:1.5em}h1{font-size:3em;line-height:1.125em}h2{font-size:2.25em;line-height:1.5em}#top,h3{font-size:1.5em;line-height:2em;font-weight:700}h4{font-size:1em;line-height:1.5em;font-weight:700}h5{line-height:1.5em;font-weight:700}a,a:visited{color:#050505}a:focus,a:hover{text-decoration:none}a:focus.purple,a:hover.purple{color:#cd9aba}a:focus.blue,a:hover.blue{color:#8baecc}a:focus.green,a:hover.green{color:#a4c39c}a:focus.yellow,a:hover.yellow{color:#e3cf56}nav a,nav a:visited{text-decoration:none;color:#050505}.classification header>nav a:focus,.classification header>nav>ul>li>a:hover{color:#881058}.regression header>nav a:focus,.regression header>nav>ul>li>a:hover{color:#0e5f9b}.techniques header>nav a:focus,.techniques header>nav>ul>li>a:hover{color:#277512}.summary header>nav a:focus,.summary header>nav>ul>li>a:hover{color:#dbbd0b}.home header>nav a:focus,.home header>nav>ul>li>a:hover,.listen header>nav a:focus,.listen header>nav>ul>li>a:hover{color:#c7c7c7}em{font-style:italic}dl dt,h1,h2,strong{font-weight:700}code,pre{font-family:prestige-elite-std,Menlo,monospace}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}blockquote{margin:10px 50px 10px 0;padding-left:15px;border-left:3px solid #c7c7c7}.classification blockquote{color:#881058;border-color:#881058}.regression blockquote{color:#0e5f9b;border-color:#0e5f9b}.techniques blockquote{color:#277512;border-color:#277512}.summary blockquote{color:#dbbd0b;border-color:#dbbd0b}header *{font-family:proxima-nova,HelveticaNeue,"Helvetica Neue",Helvetica,Verdana,Arial,sans-serif;font-weight:900}.left{text-align:left}.right{text-align:right}.center,figure,figure figcaption,figure figcaption p{text-align:center}.f-l{float:left}#top,.f-r{float:right}.c-l{clear:left}.c-r{clear:right}.c-b{clear:both}.i-b{display:inline-block}.a-c,header ul{text-transform:uppercase}.s-c{font-variant:small-caps}body{background:#f3f3f3}body.classification{background:#faf4f8}body.regression{background:#edf1f6}body.techniques{background:#f3f6f2}body.summary{background:#f9f9f4}.container{margin:0 auto;width:960px}.post-html{width:640px;margin:0 auto;padding:0 0 36px}.post-html ol{list-style:decimal-leading-zero}.post-html ol li{padding-bottom:10px;line-height:1.4em}.post-html ul{float:none;list-style:disc}.post-html ul li{padding-bottom:10px;line-height:1.4em}.post-html ul li>ul{padding-left:24px;padding-top:6px}.post-html ul li>ul>li{list-style:circle;padding-bottom:4px}header{margin:10px 0 26px;padding:0 40px}header *{display:inline-block}.titles{position:relative;margin-top:-460px}footer{height:18px;margin:18px 0 36px;display:block;width:960px}figure{padding:0 0 18px;margin:0 auto}figure img{padding:9px;width:520px}figure figcaption{width:100%;font-size:75%;color:#949494;position:relative}#top{position:fixed;bottom:24px;right:12%}nav{display:inline-block;height:80px;float:left}nav li{cursor:pointer}header ul{float:left}header ul>li{display:inline-block;padding:32px 0 0 24px;position:relative}header ul>li>ul{float:right;height:24px;margin:-9px 16px 0}header ul>li>ul li{padding:0 16px 0 0}header ul>li>ul li .classification{color:#cd9aba}header ul>li>ul li .regression{color:#8baecc}header ul>li>ul li .techniques{color:#669c57}header ul>li>ul li .summary{color:#e3cf56}header ul>li>ul li:hover .classification{color:#ab568a}header ul>li>ul li:hover .regression{color:#4d86b4}header ul>li>ul li:hover .techniques{color:#277512}header ul>li>ul li:hover .summary{color:#dbbd0b}dl{clear:both}dl .defn-container{float:left;width:480px;height:80px;border-left:4px solid #272727;padding:9px 0 9px 9px;margin:9px;background:#fff;-webkit-border-top-right-radius:3px;-webkit-border-bottom-right-radius:3px;-moz-border-radius-topright:3px;-moz-border-radius-bottomright:3px;border-top-right-radius:3px;border-bottom-right-radius:3px}dl .defn-container.classification{border-color:#881058}dl .defn-container.regression{border-color:#0e5f9b}dl .defn-container.techniques{border-color:#277512}dl .defn-container.summary{border-color:#dbbd0b}dl dt{margin-bottom:18px;width:440px}dl dd{font-size:.75em;line-height:1.125em;width:440px}dl .defn-arr{padding-top:32px;float:right;display:inline-block}dl .defn-arr a{text-decoration:none}dl:after{clear:left}.vertical-bar{position:relative;top:5px;height:24px;width:4px;margin-right:6px;display:inline-block}.vertical-bar.tall{position:static;height:80px;margin:0 16px}.vertical-bar.dark-grey{background:#272727}.vertical-bar.classification{background:#cd9aba}.vertical-bar.regression{background:#8baecc}.vertical-bar.techniques{background:#669c57}.vertical-bar.summary{background:#e3cf56}a.purple:hover .vertical-bar{background:#ab568a}a.blue:hover .vertical-bar{background:#4d86b4}a.green:hover .vertical-bar{background:#277512}a.yellow:hover .vertical-bar{background:#dbbd0b}hr{border:none;border-top:1px solid #3e3e3e}.classification hr{border-color:#881058}.regression hr{border-color:#0e5f9b}.techniques hr{border-color:#277512}.summary hr{border-color:#dbbd0b}table{border-collapse:collapse;border-spacing:0;empty-cells:show;border:2px solid #3e3e3e;margin:15px auto}.classification table{border-color:#881058}.regression table{border-color:#0e5f9b}.techniques table{border-color:#277512}.summary table{border-color:#dbbd0b}table caption{color:#000;font:italic 85%/1 arial,sans-serif;padding:1em 0;text-align:center}table td,table th{border-width:0 0 0 1px;font-size:inherit;text-align:center;margin:0;overflow:visible;padding:.5em 1em}table td:first-child,table th:first-child{border-left-width:0}table thead{text-align:left;vertical-align:bottom}.classification table thead{background-color:#881058;color:#faf4f8}.regression table thead{background-color:#0e5f9b;color:#edf1f6}.techniques table thead{background-color:#277512;color:#f3f3f3}.summary table thead{background-color:#dbbd0b;color:#f3f3f3}table td{background-color:transparent}.classification table tr:nth-child(2n) td{background-color:#cd9aba}.regression table tr:nth-child(2n) td{background-color:#8baecc}.techniques table tr:nth-child(2n) td{background-color:#669c57}.summary table tr:nth-child(2n) td{background-color:#e3cf56}@-webkit-keyframes fadeInLeft{0%{opacity:0;-webkit-transform:translate3d(-16px,0,0);transform:translate3d(-16px,0,0)}100%{opacity:1;-webkit-transform:none;transform:none}}@keyframes fadeInLeft{0%{-webkit-transform:translate3d(-16px,0,0);transform:translate3d(-16px,0,0)}100%{opacity:1;-webkit-transform:none;transform:none}}.fadeInLeft{-webkit-animation:fadeInLeft .4s;animation:fadeInLeft .4s;-webkit-animation-fill-mode:both}
  </style>

  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->


</head>

    <header class="f-l">
  <img src="/MERXAMP/assets/images/mer-main-logo-bw.png" alt="MER Machine Learning" class="f-l" height="80" width="300">
  <span class="vertical-bar tall dark-grey f-l"></span>
  <nav>
    <ul>
      <li>
        <a href="/MERXAMP/">
          Home
        </a>
      </li>
      <li>
        Articles
        <ul>
          <li>
            <a href="/MERXAMP/articles/techniques" class="green techniques">
              <span class="vertical-bar techniques"></span> Techniques
            </a>
          </li>
          <li>
            <a href="/MERXAMP/articles/classification" class="purple classification">
              <span class="vertical-bar classification"></span> Classification
            </a>
          </li>
          <li>
            <a href="/MERXAMP/articles/regression" class="blue regression">
              <span class="vertical-bar regression"></span> Regression
            </a>
          </li>
          <li>
            <a href="/MERXAMP/articles/summary" class="yellow summary">
              <span class="vertical-bar summary"></span> Summary
            </a>
          </li>
        </ul>
      </li>
    </ul>
  </nav>
</header>
<div class="c-b"></div>
  <div class="container">
    <div id="post-regression" class="post-html"><h1>Regression</h1>

<hr>

<h3>Introduction</h3>

<p>Music emotion recognition (MER) is a complicated problem to resolve, but a lot of research is going into using different methodologies to reason about it. This article will focus on one methodology, <em>regression</em>, to help a machine effectively understand emotions connected to different kinds of music. Regression places data values on a continuous plane. In terms of MER, this plane contains many different emotions which have having a certain mood (determined by a Valence level) and intensity (determined by an Arousal level). For example, an excited mood will have a positive valence level with a very high arousal level whereas a sleepy mood will have a slightly negative valence level with a low arousal level. These moods are associated with specific segments of a 2D plane, with valence on the horizontal axis and arousal on the vertical axis. (see <a href="#Fig41">Fig. 4.1</a>) We shall refer to this as the Arousal-Valence plane of emotion (Thayer, 1989).</p>

<h4 id="Fig41" class="center">Valence-Arousal Diagram</h4>

<p></p><figure markdown="1">
<img src="nanaImages/VADiagram.png" alt="Thayer's two-dimensional emotion plane"><p></p>

<figcaption>

<p>Figure 4.1: Adapted from: Russell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social Psychology, 39, 1161–1178.</p>

</figcaption>

<p></p></figure><p></p>

<p>Using regression, it is a machine's job to map musical samples to their correct emotion on this plane. Another aspect of MER currently being looked into is the ability to improve how a machine analyses emotions over time. Throughout this article, I will look into the importance of communication between machines and humans, psychological issues related to emotions and properties of regression. I will then compare techniques that use regression and analyse how an MER machine can improve its evaluation process.</p>

<h3 id="MisC">Miscommunication</h3>

<p>An interesting issue to consider when labelling a music sample with an emotion is the fact that not everyone will label the same music with the same emotion. For example, elderly people may not appreciate hip hop music as much as their younger counter parts, thus there is a concern that MER machines will provide conflicting emotions for certain samples. Yang et al. (2008) noted problems with applying certain adjectives to music as they can be ambiguous. Using ambiguous emotions will lead to miscommunication between the machine and the user when it comes to selecting a music sample for a specific emotion.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref">1</a></sup></p>

<h3 id="PsyIm">Psychological Impact</h3>

<p>Another problem we have to consider is the fact that different people will perceive different emotions for a music sample based on the experiences they had whilst listening to the music. Michael Stevens (2013) made a video noting the effects of nostalgia, and how music triggers different emotions because of certain memories. He goes on to state: “Music is initially processed in the same regions of the brain that process memories and emotion…”.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref">2</a></sup> Because of this, if something traumatic happens to you whilst listening to a music sample that could be happy in nature, you may perceive it as sad and distressing because it unearths memories you would rather not think about. This is a very subjective topic which machines will not be able to comprehend very well, even humans don’t fully understand the link between music, emotion and memories. These psychological issues make regression complicated to use as machines currently can’t perceive abstract concepts such as memories, thus they will struggle to adapt to problems involving them.</p>

<h3>Continuous vs. Discrete</h3>

<p>When it comes to working with the Arousal-Valence plane there are a few problems researchers have to address. For example, the top right quadrant will contain areas for emotions such as happy, excited and thrilled. Despite their similarities they are still distinctively different emotions. Because of this, when working with the selection of a music sample's emotion on the Arousal-Valence plane, some researchers consider a continuous spectrum of emotions (Yang et al., 2008). Though it is significantly easier for humans to select emotions discretely (e.g. stating an emotion is happy rather than a coordinate on a plane), having each point measured in continuous quantities greatly eases the ambiguity issue.</p>

<h3 id="TimeCont">Time Intervals</h3>

<p>The emotion of music can greatly vary over time. A piece of music could express a sad emotion, then change to a joyful one later on in the song. This problem can be remedied if we consider the emotions of different intervals of a piece of music, rather than one emotion for the whole duration.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref">3</a></sup> The <abbr title="Arousal-Valence">AV</abbr> values, corresponding to points on Thayer’s Arousal-Valence Plane, of set time intervals of a music sample (say 5 seconds) is recorded throughout the duration of the song. A song with a duration of 60 will have 12 associated <abbr title="Arousal-Valence">AV</abbr> values for each time interval (If a 5 second interval is used). A mean <abbr title="Arousal-Valence">AV</abbr> value is calculated from each interval’s <abbr title="Arousal-Valence">AV</abbr> values and used to calculate the overall emotion of the song.</p>

<h3>Support Vector Regression</h3>

<p>There are quite a few systems in place that uses regression to analyse emotion in music. Han B. et al. (2009) researched the benefits of using a support vector regression (<abbr title="Support Vector Regression.">SVR</abbr>) technique to perceive the emotion of music. They took a categorical approach by splitting the Arousal-Valence axis into 11 categories of emotions.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref">4</a></sup> Their <abbr title="Support Vector Regression.">SVR</abbr> system analyses seven musical features: pitch, tempo, volume, tonality, key, rhythm and harmonics. The <abbr title="Support Vector Regression.">SVR</abbr> system processes a specific music sample’s musical features and maps each feature to certain emotion categories on the Arousal-Valence plane such as calm, nervous, exited and sad. The system then creates vectors out of the evaluated musical features depending on the emotion each feature expressed. These vectors are mapped onto a polar coordinate system (similar in nature to the Cartesian Arousal-Valence plane). The <abbr title="Support Vector Regression.">SVR</abbr> is able to effectively determine the mood of musical features by evaluating the categories on the Arousal-Valence plane their vector representation fall upon.
</p><figure markdown="1">
<img src="/nanaImages/SVRFlowChart.png" alt="Flow Chart"><p></p>

<figcaption>
  Figure 4.2: SVR flow chart
</figcaption>

<p></p></figure><p></p>

<h3 id="ActLearning">Active Machine Learning</h3>

<p>Support vector regression could be improved if more sophisticated machine learning elements were integrated into <abbr title="Support Vector Regression.">SVR</abbr> systems. If the results of music emotion evaluations were stored, the system could use this bank when determining the nature of a new data piece. This method of machine learning is usually referred to as Active Learning. Simon Tong (2001) investigated the benefits of active learning. He noted that when analysing large sets of data, it can be quite time consuming for a machine to look through all features.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref">5</a></sup> Active learning features can allow machines to query results from previous data to evaluate future data. This vastly improves a MER machine as it is able to produce results with greater speed and learn how to analyse data more effectively at the same time.</p>

<h3>Linear Regression</h3>

<p>Linear regression (<abbr title="Linear Regression.">LR</abbr>) is another technique that utilises regression. Yu-An Chen et al. (2014) used a linear regression based learner for their MER machine.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref">6</a></sup> As noted before, different people can give varying emotions for the same music. Because of this, <abbr title="Linear Regression.">LR</abbr> usually models the emotions a music sample expresses through Gaussian distribution. Techniques involving Gaussian distributions are other ways of mapping musical signals to their respective points on an Arousal-Valence plane. Yang et al. (2012) produced a technique called Acoustic Emotion Gaussians (<abbr title="Acoustic Emotion Gaussians.">AEG</abbr>) which can be used to train a <abbr title="Linear Regression.">LR</abbr> machine.<sup id="fnref:9"><a href="#fn:9" class="footnote-ref">7</a></sup> This approach creates a set of acoustic features that, in MER, would map on to a set of <abbr title="Arousal-Valence">AV</abbr> values with the same number of elements using <abbr title="Acoustic Emotion Gaussians.">AEG</abbr>. The <abbr title="Arousal-Valence">AV</abbr> values from the second set are used to plot points on the Arousal-Valence plane which is then used to determine an overall emotion.</p>

<h3>Regression Techniques Comparison</h3>

<p>I have looked into two techniques that use regression to implement MER machines.  They are similar in the fact they both use a continuous model and Thayer’s Arousal-Valence plane. The table below will list a few advantages and disadvantages tied to these techniques.</p>

<h4 class="center">Linear Regression</h4>

<table>
<thead>
<tr>
  <th>Advantages</th>
  <th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Works well with speech recognition when little data input is used.<sup id="fnref2:3"><a href="#fn:3" class="footnote-ref">6</a></sup> This will aid the machine when analysing vocal features.</td>
  <td>Support Vector Regression has a higher prediction accuracy.<sup id="fnref2:2"><a href="#fn:2" class="footnote-ref">1</a></sup></td>
</tr>
<tr>
  <td></td>
  <td><abbr title="Acoustic Emotion Gaussians.">AEG</abbr> is still in the process of implementing more features (such as lyric analysis) to aid Linear Regression systems.<sup id="fnref2:9"><a href="#fn:9" class="footnote-ref">7</a></sup></td>
</tr>
</tbody>
</table>

<h4 class="center">Support Vector Regression</h4>

<table>
<thead>
<tr>
  <th>Advantages</th>
  <th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Active learning could be used alongside an <abbr title="Support Vector Regression.">SVR</abbr> learner to improve its evaluation process.</td>
  <td>The use of discrete emotion segments on the Arousal-Valence plane introduces a level of ambiguity which can prove problematic.</td>
</tr>
<tr>
  <td><abbr title="Support Vector Regression.">SVR</abbr> use of a polar plane to represent the Arousal-Valence plane has greatly increased the accuracy of MER machines.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref">8</a></sup></td>
  <td></td>
</tr>
</tbody>
</table>

<h3>Conclusion</h3>

<p>Regression has proven itself to be a very useful methodology when it comes to giving machines the necessary tools to reason about emotions and music. It addresses problems on clarity of results (miscommunication between users) through a continuous plane. Regression helps improve the applications of MER machines by implementing features such as time interval analysis. <abbr title="Support Vector Regression.">SVR</abbr> and <abbr title="Linear Regression.">LR</abbr> are regression techniques which apply quite different processes. They both show promising results brought about by the implementation of regression.<sup id="fnref2:5"><a href="#fn:5" class="footnote-ref">4</a></sup> <sup id="fnref3:3"><a href="#fn:3" class="footnote-ref">6</a></sup> These technique show the possibility for advancements in the incorporation of machine learning algorithms such as those which use active learning. Though the practice of using ML algorithms in MER is in its early stages, it is clear that ML could go on to improve our understanding of the relationship between music and emotions.</p>

<h3>References</h3>

<div class="footnotes">
<hr>
<ol>

<li id="fn:2">
<p>Yang , Y., Lin , Y., Su , Y. &amp; Chen , H. (2008) A Regression Approach to Music Emotion Recognition. <em>IEEE Transactions on Audio, Speech, and Language Processing.</em> 16 (2), 1-1-10.&nbsp;<a href="#fnref:2" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:2" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:1">
<p>Stevens, M. (2013) <em>Why Do We Feel Nostalgia?</em> [Video] Available from: <a href="https://www.youtube.com/watch?v=coGfGmOeLjE">https://www.youtube.com/watch?v=coGfGmOeLjE</a> [Accessed 15/02/2015].&nbsp;<a href="#fnref:1" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:6">
<p>Kim, Y., Schmidt , E., Migneco, R., Morton, B., Richardson , P., Scott , J., Speck , J. &amp; Turnbull , D. (2010) <em>Music Emotion Recognition: A State Of The Art Review.</em> Computer Science. Drexel University.&nbsp;<a href="#fnref:6" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:5">
<p>Han , B., Rho , S., Dannenberg , R. &amp; Hwang , E. (2009) Smers: Music emotion recognition using support vector regression. <em>10th International Society for Music Information Retrieval Conference.</em>.&nbsp;<a href="#fnref:5" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:5" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:7">
<p>Tong , S. (2001) <em>Active Learning: Theory and Applications.</em> Philosophy. Stanford university.&nbsp;<a href="#fnref:7" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:3">
<p>Chen, Y., Wang, J., Yang, Y. &amp; Chen, H. (2014) Linear regression-based adaptation of music emotion recognition models for personalization. <em>2014 IEEE International Conference on Acoustic, Speech and Signal Processing.</em> [Online], 09/03/2015. Available from: <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6853979&amp;tag=1">http://ieeexplore.ieee.org/stamp/stam…</a> [Accessed 09/03/2015].&nbsp;<a href="#fnref:3" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:3" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref3:3" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:9">
<p>Wang, J., Yang, Y., Wang, H. &amp; Jeng, S. (2012) <em>The acoustic emotion Gaussians model for emotion-based music annotation and retrieval.</em> Department of Electrical Engineering. National Taiwan University.&nbsp;<a href="#fnref:9" class="footnote-backref"><i class="icon-up-right small"></i></a> <a href="#fnref2:9" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

<li id="fn:10">
<p>Barthet, M., Fazekas, G. &amp; Sandler, M. (2013) <em>Music Emotion Recognition: From Content- to Context- Based Models.</em> Centre of Digital Music. Queen Mary University of London.&nbsp;<a href="#fnref:10" class="footnote-backref"><i class="icon-up-right small"></i></a></p>
</li>

</ol>
</div>
</div>

<a href="#" id="top">
  ↑
</a>
    <div class="c-b"></div>
<footer>
  <span class="f-l">© Group 21.</span>
  <span class="f-r right">MER Machine Learning</span>
</footer>
  </div>



</html>
